# speech_recognition_researches
Здесь я буду писать о моих продвижениях в теме "Speech recognition".

### 19/04/19 - прочитал [статью 1](https://habr.com/ru/post/226143/) на хабре
  #### Основные идеи: 
       1) Тяжело анализировать звук в координатах амплитуда-время => используют преобразование Фурье для того, чтобы перейти к координатам амплитуда-частота(для сглаживания используется оконная функция Хэмминга).
       2) Так как звук воспринимается человеком субъективно => вводят некоторое преобразование над частотой(mel-фильтры), которое основано на субъективном восприятии среднестатистическими людьми..
       3) Для модификации признаков(mel-коэффициентов) можно использовать логарифмирование энергии спекста, косинусное преобразование и т.д.
       4) В качестве алгоритма распознавания автор использовал Скрытые Марковские Модели(СММ).
   #### Итог: 65% верных распознаваний(На нескольких записей чисел от 0 до 9).
### 21/04/19 - прочитал [статью 2](https://towardsdatascience.com/ok-google-how-to-do-speech-recognition-f77b5d7cbe0b) на towardsdatascience
  #### Основные идеи, содержание:
    1) Используется библиотека librosa(python) для чтения .wav файлов. Размер выходного массива определеяется Sampling Rate
    2) Какие признаки важны и как их получить?
      * Перевод сигнала на другие оси - от амплитуда-время к амплитуда-частота с помощью преобразования Фурье(за счет этого еще экономится память!)
      * Строятся спектрограммы - специальные картинки с x - время, y - частота + цвет показывает амплитуду в данный момент времени при данной частоте
     3) Описывается Стохастический Градиентный спуск с рестартами. В двух словах - в каждой эпохе learning rate убывает, а в начале следующей - восстанавливается до начального значения.
     4) Snapshot Ensembling(дословно - ансамбль снимков) заключается в расстановке чекпоинтов для градиентного спуска, а затем усреднения всех результатов на этих чекпоинтах.
     5) Рассказывается о лайфхаке для google Colaboratory для того, чтобы ускорить обучение модели.
  #### Итог: 90% точности на соревнований - [Tensorflow Speech Recognition Challenge](https://www.kaggle.com/c/tensorflow-speech-recognition-challenge)

### 05/05/19 - прочитал [статью 3](https://towardsdatascience.com/speech-recognition-is-hard-part-1-258e813b6eb7) на towardsdatascience - больше ознакомительная, никаких методов не было описано, но обещали 2 часть, где будет конкретика(пока нет)
  #### Основные идеи, содержание:
      1) Есть несколько видов моделей распознавания аудио: 
        * Акустические модели(Acoustic models) - эта модель делит аудио на маленькие части предсказывает для них самые подходящие фонемы
        * Модели произношения(Pronunciation models) - соединяет части в слова и сопостовляет каждому слову его фонетическое представление
        * Языковые модели(Language models) - соединяет слова вместе и выбирает наиболее вероятную последовательность слов среди множества таких же последовательностей.
      2) Все модели в основном используют комбинацию из этих всех подходов.
### 06/05/19 - начал участвовать в [Freesound-audio-tagging-2019](https://www.kaggle.com/c/freesound-audio-tagging-2019/overview/submission-modes).
  #### Прогресс:
    1) Изучаю кернелы, очень интересный [кернел](https://www.kaggle.com/maxwell110/beginner-s-guide-to-audio-data-2). В нем реализованы 1D-conv и 2D-conv сети с MFCC-коэффициентами.
    
### TODO: 
      1) Прочитать про Скрытые Марковские Модели
      2) Поучаствовать в соревновании - https://www.kaggle.com/c/freesound-audio-tagging-2019/overview/submission-modes
